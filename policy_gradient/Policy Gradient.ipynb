{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient\n",
    "\n",
    "GOLAI Policy Gradient attempt ðŸ™ƒ.\n",
    "\n",
    "Policy gradients formula:\n",
    "\n",
    "$$\\pi_\\theta(s,a)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\theta$: is the parameter vector,\n",
    "- $s$: is a particular state,\n",
    "- $a$: is an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "from game.GOLAI.arena import Arena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(16, 128) # Input shape: state\n",
    "        self.affine2 = nn.Linear(128, 2) # Output shape: actions\n",
    "\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    " \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    # Get state in tensor shape (1, ...)\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "\n",
    "    # Get action by running policy model and choosing based on probabilities in state\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "\n",
    "    # Save action\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode(gamma):\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    rewards = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.tensor(rewards)\n",
    "    rewards = (reward - rewards.mean()) / (rewards.std() + eps)\n",
    "    for log_prob, reward in zip(policy.save_log_probs, rewards):\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.save_log_probs[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(log_interval):\n",
    "    running_reward = 10\n",
    "    for i_episode in count(1):\n",
    "        for t in range(10000):\n",
    "            action = select_action(state)\n",
    "            # Play the game\n",
    "            end_round = play_round(state_new, None)\n",
    "            score = calculate_score(end_round)\n",
    "            done = (t == horizon - 1) # When all tiles were placed, we are done\n",
    "            D.append((state, action, score, state_new, done))\n",
    "            policy.rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            running_reward = running_reward * 0.99 + t * 0.01\n",
    "            finish_episode()\n",
    "            if i_episode % log_interval == 0:\n",
    "                print(\"Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}\".format(i_episode, t, running_reward))\n",
    "            if running_reward > reward_threshold: # ???\n",
    "                print(\"Solved! Running reward is now {} and the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Policy gradient RL in Pytorch](https://medium.com/@ts1829/policy-gradient-reinforcement-learning-in-pytorch-df1383ea0baf)\n",
    "- [Reinforce.py](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
